# Predicting Interview Attendance
Using a [Kaggle dataset](https://www.kaggle.com/vishnusraghavan/the-interview-attendance-problem/data), this project develops an application that performs the following steps in order to predict if a candidate will attend an interview.
1. Preprocesses the raw data by cleaning and standardizing data fields that can be used as features in a predictive model. Creates two CSV files of labeled and unlabeled data (`preprocess/clean_data.py`).
2. Splits the labeled dataset into a training and test set using an 80%/20% split. Trains and evaluates a classifier (i.e. SVM) with scikit-learn (`model/model.py`) using cross-validation for hyper-parameter tuning. Predicts interview attendance for candidates in the unlabeled dataset and writes the prediction results to a CSV file (`model/predict.py`).

## Setup
### Clone the project repo
`git clone git@github.com:jessdaubner/attendance-predictor.git`

### Build & Run the App
Download and install [Docker](https://www.docker.com/get-started) and build the app locally:
```bash
cd attendance-predictor
docker build -t attendance-predictor .
```

Running the container will execute all steps necessary to build the predictive model and generate predictions on the unlabeled data:
```
docker run --rm -ti attendance-predictor
```
NOTE: This will overwrite some of the existing data files and model artifacts in the container.

### Running Jupyter Notebooks
To explore the raw or cleaned CSV files in the `predictor/data/` including `labeled.csv`, `unlabeled.csv`, and `predictions.csv`, launch a notebook from the container:
1. Run the container as follows:
```bash
docker run -it -p 8888:8888 attendance-predictor
```
2. At the command prompt inside the container:
```bash
cd predictor
jupyter notebook --ip 0.0.0.0 --no-browser --allow-root
```
3. Follow the prompt (`To access the notebook, open this file in a browser:...`) and copy the URL into a browser tab.

### Development
To make development faster, volume mount the `app` directory over the one in the container:
```bash
docker run --rm -v `pwd`/predictor:/predictor -ti attendance-predictor /bin/bash
```
This enables edits made locally to be reflected in the container and be quickly tested and run.

### Testing
To run unit tests, run `pytest` in the repository.
```
root@4689da80e66d:/predictor# pytest
========================================================================= test session starts =========================================================================
platform linux -- Python 3.7.3, pytest-4.4.1, py-1.8.0, pluggy-0.9.0
rootdir: /predictor
collected 6 items

tests/preprocess/test_clean_data.py ......                                                                                                                      [100%]

====================================================================== 6 passed in 0.39 seconds =======================================================================
```

## Model

### Evaluating Model Performance
A client has scored a candidate with your model and it gave the candidate a 30% chance of attending the interview. However, the candidate did come to the interview. The client would like to know why there is this apparent discrepancy in your model. How would you explain this occurrence? Could you provide a better way for the client to evaluate your model's performance?

Probability is a measure of how likely an event is to occur. However, the interpretation of specific probabilities can be subjective depending on the person interpreting the value. For example, a probability of 30% could be interpreted as falling into different areas of possiblity, such as "probably not" or "highly doubtful" depending on the interpreter if the information is provided in a context that lacks agreed upon definitions mapping mathematical odds to commonly used phrases of probability. While a 30% chance of attendance may be interpreted by the hiring manager as meaning the candidate is highly unlikely to attend, another perspective  is that over the long-run given a candidate and position with the same set of attributes, we'd expect the candidate to attend the interiew approximately 30 times out of 100.

Given the inherent subjectivity in interpreting probabilities for disparate use cases, constructing a confusion matrix for the model enables us to better understand and evaluate model performance than the probability of attendance generated by the model alone. The confusion matrix tabulates the false positives, false negatives, true positives, and true negatives generated by the model, which we can then use to calculate more meaningful statistics such as precision and recall for the model. For example, if users are particularly sensitive to expected "no-show" candidates arriving onsite, that is, false negatives, we will want to track recall and attempt to lower the probability threshold used to predict attendance.


## Resources
* [Words of Estimative Probability - CIA Library](https://www.cia.gov/library/center-for-the-study-of-intelligence/csi-publications/books-and-monographs/sherman-kent-and-the-board-of-national-estimates-collected-essays/6words.html)
### Scikit Learn
* [Parameter estimation using grid search with cross-validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html)
